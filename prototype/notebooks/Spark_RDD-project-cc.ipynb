{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project CCF pySpark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkConf,SparkContext\n",
    "import os\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('pyspark-shell')\n",
    "conf.set('spark.driver.host', '127.0.0.1')\n",
    "conf.set(\"spark.ui.proxyBase\", \"\")\n",
    "conf.set(\"spark.cores.max\",\"4\")\n",
    "conf.set(\"spark.executor.cores\",\"2\")\n",
    "os.environ['PYSPARK_PYTHON'] = '/Users/ccompain/.pyenv/versions/miniconda3-latest/bin/python' # Needs to be explicitly provided as env. Otherwise workers run Python 2.7\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter' \n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCF_dedup(data):\n",
    "  dedup = data.map(lambda x: ((x[0],x[1]), 1))\\\n",
    "              .reduceByKey(lambda x,y: 1)\\\n",
    "              .map(lambda x: (x[0][0], x[0][1]))\n",
    "#              .sortBy(lambda x:x[0])\n",
    "  return dedup\n",
    "\n",
    "def CCF_Iterate_map(pair):\n",
    "    #map\n",
    "    pair2=pair.map(lambda x: (x[1], x[0]))\n",
    "    map_pair=pair.union(pair2) \n",
    "    #map_pair=pair.union(pair2).sortByKey(ascending=True)\n",
    "    return map_pair\n",
    "\n",
    "def f(x): return x\n",
    "    \n",
    "def CCF_Iterate_reduce(data):\n",
    "    #find min value per key\n",
    "    #couple (key, min)\n",
    "    key_min=data.reduceByKey(lambda x,y: min(x,y)).filter(lambda x: x[0]>x[1])\n",
    "    #put together: in one command\n",
    "    valuelist_min=key_min.join(data).map(lambda x: (x[0],x[1][1])).cogroup(key_min).map(lambda x :(x[0], ( list(x[1][0]), list(x[1][1]))))\n",
    "    \n",
    "    #make RDD : (min, (list of value))\n",
    "    min_valuelist=valuelist_min.map(lambda x:(x[1][1][0], x[1][0]))\n",
    "\n",
    "    #use flatmapvalue to transform to RDD (min, value), then filter min != value, map to couple (value, min)\n",
    "\n",
    "    min_value=min_valuelist.flatMapValues(f).filter(lambda x: x[0]!=x[1]).map(lambda x: (x[1],x[0]))\n",
    "    countnewpair=min_value.count()\n",
    "\n",
    "    #union couple (key, min) and (value, min)\n",
    "    #unionkeyminvalue=key_min.union(min_value).sortBy(lambda x:x[1])\n",
    "    unionkeyminvalue=key_min.union(min_value)\n",
    "\n",
    "    return unionkeyminvalue\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('A', 'B'), ('B', 'C'), ('B', 'D'), ('D', 'E'), ('F', 'G'), ('G', 'H')]"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "r=sc.parallelize([(\"A\",\"B\"),(\"B\",\"C\"),(\"B\",\"D\"),(\"D\",\"E\"),(\"F\",\"G\"),(\"G\",\"H\")])\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize flags\n",
    "new_pair_flag = True\n",
    "iteration = 0\n",
    "new_reduce = r\n",
    "current_size = new_reduce.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "################################\n Start CCF RDD\n--------------------------------\nNumber of pairs :6\n*** Iteration 1 ***\n--> Iteration 1 : Number of new pairs = 4\n*** Iteration 2 ***\n--> Iteration 2 : Number of new pairs = 9\n*** Iteration 3 ***\n--> Iteration 3 : Number of new pairs = 4\n*** Iteration 4 ***\n--> Iteration 4 : Number of new pairs = 0\n*** Stop the loop ***\nClean the last RDD of duplicate pairs\n--------------------------------\nTotal iterations : 4\n################################\n"
    }
   ],
   "source": [
    "print(\"################################\")\n",
    "print(\" Start CCF RDD\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Number of pairs :{current_size}\")\n",
    "while new_pair_flag:\n",
    "    iteration +=1\n",
    "    print(f\"*** Iteration {iteration} ***\")\n",
    "    new_map = CCF_Iterate_map(new_reduce)\n",
    "    new_reduce_tmp = CCF_Iterate_reduce(new_map)\n",
    "    new_size = new_reduce_tmp.count()\n",
    "    print(f\"--> Iteration {iteration} : Number of new pairs = {new_size-current_size}\")\n",
    "    if (new_size-current_size)>0:\n",
    "        new_reduce = CCF_dedup(new_reduce_tmp)\n",
    "    else:\n",
    "        print(\"*** Stop the loop ***\")\n",
    "        print(\"Clean the last RDD of duplicate pairs\")\n",
    "        new_reduce = CCF_dedup(new_reduce_tmp)\n",
    "        #print(f\"Save last RDD in {output_directory}\")\n",
    "        #new_reduce.coalesce(1).saveAsTextFile(output_directory)\n",
    "        new_pair_flag = False\n",
    "print(\"--------------------------------\")\n",
    "print(\"Total iterations :\",iteration)\n",
    "#print(\"Results dump : \")\n",
    "#subprocess.call([\"hdfs\", \"dfs\", \"-ls\", output_directory])\n",
    "print(\"################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('G', 'F'), ('E', 'A'), ('H', 'F'), ('D', 'A'), ('B', 'A'), ('C', 'A')]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "new_reduce.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "name": "Spark_RDD project",
  "notebookId": 3156448150564147
 },
 "nbformat": 4,
 "nbformat_minor": 1
}