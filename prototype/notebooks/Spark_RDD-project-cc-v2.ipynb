{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkConf,SparkContext\n",
    "import os\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('pyspark-shell')\n",
    "conf.set('spark.driver.host', '127.0.0.1')\n",
    "os.environ['PYSPARK_PYTHON'] = '/Users/ccompain/.pyenv/versions/miniconda3-latest/bin/python' # Needs to be explicitly provided as env. Otherwise workers run Python 2.7\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('A', 'B'), ('B', 'C'), ('B', 'D'), ('D', 'E'), ('F', 'G'), ('G', 'H')]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "r=sc.parallelize([(\"A\",\"B\"),(\"B\",\"C\"),(\"B\",\"D\"),(\"D\",\"E\"),(\"F\",\"G\"),(\"G\",\"H\")],2)\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCF_Iterate_reduce(pair):\n",
    "  key, values = pair\n",
    "  global accum\n",
    "  min = key\n",
    "  valueL = []\n",
    "  for value in values:\n",
    "    if value < min:\n",
    "       min = value\n",
    "    valueL.append(value)\n",
    "  if min < key:\n",
    "    yield((key, min))\n",
    "    for value in valueL:\n",
    "      if min != value:\n",
    "        accum.add(1)\n",
    "        yield((value, min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log4jLogger = sc._jvm.org.apache.log4j\n",
    "LOGGER = log4jLogger.LogManager.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pair_flag = True\n",
    "iteration = 0\n",
    "accum = sc.accumulator(0)\n",
    "dedupJob = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Iteration :1 ---  newPair : True\nIteration :2 ---  newPair : True\nIteration :3 ---  newPair : True\nIteration :4 ---  newPair : True\nIteration :5 ---  newPair : False\n"
    }
   ],
   "source": [
    "while new_pair_flag:\n",
    "    iteration += 1\n",
    "    newPair = False\n",
    "    accum.value = 0\n",
    "\n",
    "    # CCF-iterate (MAP)\n",
    "    mapJob = dedupJob.flatMap(lambda e: (e,e[::-1]))\n",
    "    #print(mapJob.collect())\n",
    "\n",
    "    # CCF-iterate (REDUCE)\n",
    "    reduceJob = mapJob.groupByKey().flatMap(lambda pair: CCF_Iterate_reduce(pair)).sortByKey()\n",
    "\n",
    "    # CCF-dedup \n",
    "    dedupJob = reduceJob.distinct()\n",
    "\n",
    "    # Force the RDD evalusation\n",
    "    tmp = dedupJob.count()\n",
    "\n",
    "    # Prepare next iteration\n",
    "    #mapJob = dedupJob\n",
    "    new_pair_flag = bool(accum.value)\n",
    "\n",
    "    #LOGGER.warn(\"Iteration :\"+str(iteration)+\" --- \"+\" newPair : \"+str(new_pair_flag))\n",
    "    print(\"Iteration :\"+str(iteration)+\" --- \"+\" newPair : \"+str(new_pair_flag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('A', 'B'), ('B', 'A'), ('A', 'D'), ('D', 'A'), ('A', 'E'), ('E', 'A'), ('F', 'G'), ('G', 'F'), ('F', 'H'), ('H', 'F'), ('A', 'C'), ('C', 'A')]\nComponent id: A | Number of nodes:  2\nComponent id: B | Number of nodes:  2\nComponent id: A | Number of nodes:  2\nComponent id: D | Number of nodes:  2\nComponent id: A | Number of nodes:  2\nComponent id: E | Number of nodes:  2\nComponent id: F | Number of nodes:  2\nComponent id: G | Number of nodes:  2\nComponent id: F | Number of nodes:  2\nComponent id: H | Number of nodes:  2\nComponent id: A | Number of nodes:  2\nComponent id: C | Number of nodes:  2\n"
    }
   ],
   "source": [
    "results = list(map(lambda e: e[::-1], mapJob.collect()))\n",
    "print(results)\n",
    "for k in results:\n",
    "    print(\"Component id:\", k[0], \"| Number of nodes: \", len(k[1]) +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}